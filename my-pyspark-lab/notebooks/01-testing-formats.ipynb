{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, lit\n\u001b[0;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      6\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelta_Iceberg_Poetry_Lab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.warehouse.dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarehouse/delta\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39menableHiveSupport() \u001b[38;5;66;03m# Habilitar suporte a Hive é bom para catálogos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Delta_Iceberg_Poetry_Lab\")\n",
    "    .config(\"spark.sql.warehouse.dir\", os.path.join(os.getcwd(), \"warehouse/delta\"))\n",
    "    .enableHiveSupport() # Habilitar suporte a Hive é bom para catálogos\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session iniciada com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Verificando Configurações ---\")\n",
    "sql_extensions = spark.conf.get(\"spark.sql.extensions\")\n",
    "spark_catalog = spark.conf.get(\"spark.sql.catalog.spark_catalog\")\n",
    "iceberg_catalog_warehouse = spark.conf.get(\"spark.sql.catalog.local_iceberg.warehouse\")\n",
    "\n",
    "print(f\"Extensões SQL ativas: {sql_extensions}\")\n",
    "print(f\"Catálogo Spark (Delta): {spark_catalog}\")\n",
    "print(f\"Warehouse do Catálogo Iceberg: {iceberg_catalog_warehouse}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Trabalhando com Delta Lake ---\")\n",
    "\n",
    "# Criar dados de exemplo\n",
    "data_delta = [\n",
    "    (1, \"Alice\", 34),\n",
    "    (2, \"Bob\", 45),\n",
    "    (3, \"Charlie\", 28)\n",
    "]\n",
    "columns_delta = [\"id\", \"nome\", \"idade\"]\n",
    "df_delta = spark.createDataFrame(data_delta, columns_delta)\n",
    "\n",
    "# Definir o nome da tabela Delta\n",
    "delta_table_name = \"funcionarios_delta\"\n",
    "\n",
    "# Salvar o DataFrame como uma tabela Delta (sobrescreve se existir)\n",
    "print(f\"Salvando dados na tabela Delta: {delta_table_name}\")\n",
    "df_delta.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name)\n",
    "\n",
    "# Ler os dados da tabela Delta\n",
    "print(\"Lendo dados da tabela Delta:\")\n",
    "spark.read.table(delta_table_name).show()\n",
    "\n",
    "# Realizar uma atualização (operação UPDATE)\n",
    "print(\"Atualizando a idade de Alice para 35...\")\n",
    "spark.sql(f\"UPDATE {delta_table_name} SET idade = 35 WHERE nome = 'Alice'\")\n",
    "\n",
    "# Mostrar o resultado após a atualização\n",
    "print(\"Dados após a atualização:\")\n",
    "spark.read.table(delta_table_name).show()\n",
    "\n",
    "\n",
    "print(\"\\n--- Trabalhando com Apache Iceberg ---\")\n",
    "\n",
    "# Criar dados de exemplo\n",
    "data_iceberg = [\n",
    "    (101, \"Produto A\", \"Eletrônicos\", 999.90),\n",
    "    (102, \"Produto B\", \"Livros\", 79.90),\n",
    "    (103, \"Produto C\", \"Eletrônicos\", 1500.00)\n",
    "]\n",
    "columns_iceberg = [\"produto_id\", \"nome\", \"categoria\", \"preco\"]\n",
    "df_iceberg = spark.createDataFrame(data_iceberg, columns_iceberg)\n",
    "\n",
    "# Definir o nome da tabela Iceberg (incluindo o catálogo)\n",
    "iceberg_table_name = \"local_iceberg.db.produtos_iceberg\"\n",
    "\n",
    "# Criar o namespace (database) se não existir\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS local_iceberg.db\")\n",
    "\n",
    "# Salvar o DataFrame como uma tabela Iceberg\n",
    "print(f\"Salvando dados na tabela Iceberg: {iceberg_table_name}\")\n",
    "df_iceberg.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(iceberg_table_name)\n",
    "\n",
    "# Ler os dados da tabela Iceberg\n",
    "print(\"Lendo dados da tabela Iceberg:\")\n",
    "spark.read.table(iceberg_table_name).show()\n",
    "\n",
    "# Adicionar uma nova coluna (evolução de esquema)\n",
    "print(\"Adicionando a coluna 'em_estoque'...\")\n",
    "spark.sql(f\"ALTER TABLE {iceberg_table_name} ADD COLUMN em_estoque BOOLEAN\")\n",
    "\n",
    "# Inserir novos dados com a coluna adicional\n",
    "spark.sql(f\"INSERT INTO {iceberg_table_name} VALUES (104, 'Produto D', 'Casa', 250.0, true)\")\n",
    "\n",
    "print(\"Dados após a evolução de esquema e inserção:\")\n",
    "spark.read.table(iceberg_table_name).show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
