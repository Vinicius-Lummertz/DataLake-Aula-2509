{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, lit\n\u001b[0;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      6\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelta_Iceberg_Poetry_Lab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.warehouse.dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarehouse/delta\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39menableHiveSupport() \u001b[38;5;66;03m# Habilitar suporte a Hive é bom para catálogos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"DeltaLakeRestaurante\")\n",
    "    .config(\"spark.sql.warehouse.dir\", os.path.join(os.getcwd(), \"../warehouse/delta\"))\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS pratos_delta\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE pratos_delta (\n",
    "    id INT,\n",
    "    nome_prato STRING,\n",
    "    categoria STRING,\n",
    "    ingredientes ARRAY<STRING>,\n",
    "    preco DECIMAL(10, 2),\n",
    "    disponivel BOOLEAN\n",
    "  ) USING delta\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  INSERT INTO pratos_delta VALUES\n",
    "  (1, 'Spaghetti Carbonara', 'Massas', ARRAY['Spaghetti', 'Ovo', 'Guanciale', 'Parmesão'], 55.00, true),\n",
    "  (2, 'Risotto de Cogumelos', 'Risotos', ARRAY['Arroz Arbóreo', 'Cogumelo Porcini', 'Vinho Branco', 'Caldo de Legumes'], 62.50, true),\n",
    "  (3, 'Bife Ancho', 'Carnes', ARRAY['Bife Ancho', 'Sal Grosso', 'Pimenta do Reino'], 89.90, false)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM pratos_delta ORDER BY id\").show(truncate=False)\n",
    "\n",
    "pratos_delta_table = DeltaTable.forPath(spark, \"./spark-warehouse/pratos_delta\")\n",
    "pratos_delta_table.history().show()\n",
    "\n",
    "spark.sql(\"UPDATE pratos_delta SET preco = 65.00 WHERE id = 2\")\n",
    "spark.sql(\"SELECT * FROM pratos_delta ORDER BY id\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"DELETE FROM pratos_delta WHERE id = 3\")\n",
    "spark.sql(\"SELECT * FROM pratos_delta ORDER BY id\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"ALTER TABLE pratos_delta ADD COLUMN chef_responsavel STRING\")\n",
    "\n",
    "spark.sql(\"UPDATE pratos_delta SET chef_responsavel = 'Chef Ana' WHERE id = 1\")\n",
    "spark.sql(\"UPDATE pratos_delta SET chef_responsavel = 'Chef Bruno' WHERE id = 2\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM pratos_delta ORDER BY id\").show(truncate=False)\n",
    "\n",
    "pratos_delta_table.history().show(truncate=False)\n",
    "\n",
    "is_delta = DeltaTable.isDeltaTable(spark, \"spark-warehouse/pratos_delta\")\n",
    "print(f\"O caminho 'spark-warehouse/pratos_delta' é uma tabela Delta? {is_delta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
