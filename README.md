Este projeto configura um ambiente de desenvolvimento local e autocontido para trabalhar com **Apache Spark (PySpark)**, **Delta Lake** e **Apache Iceberg** dentro de um **Jupyter Lab**. A gestÃ£o de dependÃªncias e do ambiente virtual Ã© feita com o **Poetry**, garantindo um setup limpo e 100% reprodutÃ­vel.

O objetivo Ã© fornecer uma "caixa de areia" para estudar, testar e desenvolver soluÃ§Ãµes de Data Lakehouse sem a necessidade de configurar um cluster complexo na nuvem.

---

## âœ¨ Tecnologias e VersÃµes

A reprodutibilidade Ã© garantida pelo uso de versÃµes especÃ­ficas das principais ferramentas:

| Tecnologia | VersÃ£o | PropÃ³sito |
| :--- | :--- | :--- |
| **Python** | `3.9+` | Linguagem base |
| **Poetry** | `1.8+` | Gerenciador de dependÃªncias |
| **Apache Spark** | `3.5.0` | Motor de processamento distribuÃ­do |
| **Jupyter Lab** | `4.x` | IDE interativa baseada na web |
| **Delta Lake** | `3.2.0` | Formato de tabela transacional |
| **Apache Iceberg** | `1.5.2` | Formato de tabela aberta para grandes datasets |
| **Java (JDK)** | `11` ou `17` | Requisito para rodar o Spark (JVM) |

---

## ğŸ“‚ Estrutura de DiretÃ³rios

O projeto estÃ¡ organizado da seguinte forma:

```
my-pyspark-lab/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01-testing-formats.ipynb   # Notebook de exemplo para testar os formatos
â”œâ”€â”€ warehouse/
â”‚   â”œâ”€â”€ delta/                     # Armazenamento local para tabelas Delta
â”‚   â””â”€â”€ iceberg/                   # Armazenamento local para tabelas Iceberg
â”œâ”€â”€ .gitignore                     # Arquivo para ignorar arquivos do Git
â”œâ”€â”€ pyproject.toml                 # Arquivo de configuraÃ§Ã£o do Poetry
â”œâ”€â”€ README.md                      # Este arquivo
â””â”€â”€ start-jupyter.sh               # Script para iniciar o ambiente
```

---

## ğŸ”§ PrÃ©-requisitos

Antes de comeÃ§ar, garanta que os seguintes softwares estejam instalados em seu sistema:

1.  **Python 3.9 ou superior**
    -   Verifique com: `python3 --version`
    -   [Instalar Python](https://www.python.org/downloads/)

2.  **Poetry**
    -   Verifique com: `poetry --version`
    -   [Instalar Poetry](https://python-poetry.org/docs/#installation) (o mÃ©todo com `curl` Ã© recomendado)

3.  **Java Development Kit (JDK) 11 ou 17**
    -   Verifique com: `java -version`
    -   O Apache Spark precisa de uma Java Virtual Machine (JVM) para ser executado.
    -   Recomendamos o [OpenJDK](https://openjdk.java.net/install/).

---

## âš™ï¸ Passo a Passo para ConfiguraÃ§Ã£o

Siga os passos abaixo para clonar e configurar o ambiente em sua mÃ¡quina local.

### 1. Clone o RepositÃ³rio

Primeiro, clone este projeto para a sua mÃ¡quina.

```bash
git clone <URL_DO_SEU_REPOSITORIO>
cd my-pyspark-lab
```

### 2. Instale as DependÃªncias Python

Use o Poetry para criar o ambiente virtual e instalar todas as bibliotecas Python listadas no arquivo `pyproject.toml`.

```bash
poetry install
```
Este comando irÃ¡ ler o arquivo `pyproject.toml`, resolver as dependÃªncias e instalÃ¡-las em um ambiente virtual isolado, especÃ­fico para este projeto.

### 3. Torne o Script de InicializaÃ§Ã£o ExecutÃ¡vel

O script `start-jupyter.sh` precisa de permissÃ£o de execuÃ§Ã£o. Este passo Ã© necessÃ¡rio apenas uma vez. (Para ambientes Linux/macOS/WSL).

```bash
chmod +x start-jupyter.sh
```

### 4. Inicie o Ambiente Jupyter Lab

Execute o script para iniciar o ambiente. Ele irÃ¡ configurar todas as variÃ¡veis de ambiente necessÃ¡rias para o Spark e depois abrir o Jupyter Lab.

```bash
./start-jupyter.sh
```
> **ğŸ’¡ Nota:** Na primeira vez que vocÃª executar este comando, o Spark levarÃ¡ alguns minutos para baixar os conectores (JARs) do Delta Lake e do Iceberg da internet. Isso Ã© normal e sÃ³ acontece uma vez.

ApÃ³s a inicializaÃ§Ã£o, o Jupyter Lab serÃ¡ aberto automaticamente no seu navegador.

### 5. Execute o Notebook de Teste

1.  No Jupyter Lab, navegue atÃ© a pasta `notebooks/`.
2.  Abra o arquivo `01-testing-formats.ipynb`.
3.  Execute todas as cÃ©lulas do notebook para confirmar que a sessÃ£o Spark Ã© criada corretamente e que vocÃª pode ler e escrever tabelas nos formatos Delta e Iceberg.

Se todas as cÃ©lulas forem executadas sem erro, seu ambiente estÃ¡ configurado e pronto para uso! âœ…

---

## ğŸ” Como Funciona?

A "mÃ¡gica" deste ambiente estÃ¡ no script `start-jupyter.sh`. Ele configura a variÃ¡vel de ambiente `PYSPARK_SUBMIT_ARGS`, que passa uma sÃ©rie de parÃ¢metros diretamente para o Spark no momento em que ele Ã© iniciado pelo Jupyter.

Os parÃ¢metros mais importantes sÃ£o:

-   `--packages`: Informa ao Spark quais JARs de projetos externos (neste caso, os conectores do Delta e do Iceberg) ele deve baixar do repositÃ³rio Maven.
-   `--conf spark.sql.extensions`: Registra as extensÃµes SQL para que o Spark entenda os novos comandos e sintaxes introduzidos por cada formato (ex: `UPDATE`, `MERGE` do Delta e `ALTER TABLE` do Iceberg).
-   `--conf spark.sql.catalog...`: Configura os catÃ¡logos do Spark. Aqui, definimos o catÃ¡logo padrÃ£o (`spark_catalog`) para ser o do Delta Lake e criamos um catÃ¡logo separado chamado `local_iceberg` para gerenciar as tabelas Iceberg, apontando para o nosso diretÃ³rio `warehouse/iceberg`.

Ao executar o script antes do Jupyter, garantimos que qualquer notebook iniciado nesta sessÃ£o jÃ¡ terÃ¡ uma sessÃ£o Spark prÃ©-configurada e pronta para usar os formatos de Lakehouse.