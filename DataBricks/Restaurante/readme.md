# Projeto Lakehouse: An√°lise de Vendas de Restaurante

Este projeto demonstra um pipeline de dados ELT (Extract, Load, Transform) completo constru√≠do no Databricks. O objetivo √© processar dados de vendas de uma rede de restaurantes, desde a ingest√£o de dados brutos at√© a cria√ß√£o de um modelo dimensional (Star Schema) pronto para an√°lise e Business Intelligence.

O pipeline utiliza a **Arquitetura Medalh√£o** (Bronze, Silver, Gold) e √© totalmente orquestrado por notebooks Databricks, garantindo reprodutibilidade e escalabilidade.

---

## üöÄ Arquitetura Medalh√£o

O pipeline √© dividido em tr√™s camadas l√≥gicas de dados, cada uma com um prop√≥sito espec√≠fico:

1.  **Camada Landing/Bronze**:
    * **Prop√≥sito**: Ingest√£o de dados brutos (Extract & Load).
    * **Processo**: Os dados de origem (simulados como CSVs) s√£o carregados de um Volume (`workspace.landing.dados`) para tabelas Delta na camada Bronze (ex: `bronze.vendas`).
    * **Transforma√ß√µes**: Nenhuma transforma√ß√£o de neg√≥cio √© aplicada. Apenas metadados de auditoria (como `data_hora_bronze` e `fonte_dados`) s√£o adicionados.

2.  **Camada Silver**:
    * **Prop√≥sito**: Dados limpos, padronizados e enriquecidos (Transform).
    * **Processo**: As tabelas da Bronze s√£o lidas e passam por um processo de limpeza.
    * **Transforma√ß√µes**: Renomea√ß√£o de colunas (ex: `id_franquia` -> `SK_FRANQUIA`), padroniza√ß√£o de tipos de dados, e valida√ß√µes de qualidade.

3.  **Camada Gold**:
    * **Prop√≥sito**: Modelo de dados de neg√≥cios, otimizado para an√°lise (BI).
    * **Processo**: Os dados da Silver s√£o agregados e modelados.
    * **Transforma√ß√µes**: Cria√ß√£o de um **Star Schema** com tabelas Fato (ex: `fato_vendas`) e Dimens√£o (ex: `dim_franquia`, `dim_cardapio`, `dim_tempo`).

---

## ‚öôÔ∏è Orquestra√ß√£o e Execu√ß√£o

O pipeline √© executado atrav√©s de uma sequ√™ncia de notebooks, que devem ser rodados na ordem correta.

### Ordem de Execu√ß√£o

1.  **`Notebooks/Setup.py`**:
    * **O que faz**: Prepara todo o ambiente. Primeiro, executa uma limpeza (`DROP SCHEMA ... CASCADE`) para garantir que o pipeline possa ser re-executado do zero.
    * **Cria√ß√£o**: `workspace.landing`, `workspace.bronze`, `workspace.silver`, `workspace.gold` e o Volume `workspace.landing.dados`.

2.  **`Notebooks/Landing-Bronze.py`**:
    * **O que faz**: Simula a ingest√£o de dados.
    * **Processo**:
        1.  Usa `dbutils.fs.put()` para criar os arquivos CSV (`vendas.csv`, `franquias.csv`, etc.) no Volume `workspace.landing.dados`.
        2.  L√™ esses CSVs com `spark.read.csv()`.
        3.  Adiciona metadados de auditoria (`data_hora_bronze`).
        4.  Salva as tabelas em formato Delta no schema `bronze`.

3.  **`Notebooks/Bronze-Silver.py`**:
    * **O que faz**: Limpa e padroniza os dados.
    * **Processo**:
        1.  L√™ as tabelas do schema `bronze`.
        2.  Aplica regras de renomea√ß√£o (ex: `id_franquia` para `SK_FRANQUIA`).
        3.  Salva as tabelas limpas em formato Delta no schema `silver`.

4.  **`Notebooks/Silver-Gold.py`**:
    * **O que faz**: Constr√≥i o modelo dimensional (Star Schema).
    * **Processo**:
        1.  L√™ as tabelas do schema `silver`.
        2.  Cria as tabelas de Dimens√£o (`dim_franquia`, `dim_profissional`, `dim_cardapio`, `dim_tempo`).
        3.  Usa `MERGE INTO` para carregar os dados nas dimens√µes (SCD Tipo 1).
        4.  Cria a tabela `fato_vendas` e a popula fazendo join com as dimens√µes para obter as chaves substitutas (SKs).

### Notebooks Auxiliares

* **`Notebooks/ShowTables.py`**: Um notebook simples de consulta para visualizar os resultados finais nas tabelas `gold.*`.
* **`Notebooks/Cleanup.py`**: Um notebook aut√¥nomo para limpar completamente todos os schemas (`landing`, `bronze`, `silver`, `gold`) do ambiente.

---

## üìä Modelo de Dados (Camada Gold)

O produto final do pipeline √© um Star Schema otimizado para consultas anal√≠ticas:

* **Tabela Fato**: `fato_vendas`
* **Tabelas de Dimens√£o**:
    * `dim_franquia`
    * `dim_profissional`
    * `dim_cardapio`
    * `dim_tempo`

---

## üìñ Documenta√ß√£o (MkDocs)

Este projeto inclui uma documenta√ß√£o detalhada gerada com MkDocs. Para visualizar:

1.  Instale o MkDocs e o tema Material:
    ```bash
    pip install mkdocs mkdocs-material
    ```
2.  Navegue at√© o diret√≥rio `DataBricks/Restaurante` (onde o `mkdocs.yml` est√°).
3.  Execute o servidor local:
    ```bash
    mkdocs serve
    ```
4.  Abra `http://127.0.0.1:8000` no seu navegador.